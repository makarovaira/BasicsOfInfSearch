import os
import numpy as np
from collections import defaultdict, Counter
from nltk.tokenize import RegexpTokenizer
from pymorphy3 import MorphAnalyzer

# â”€â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TFIDF_DIR = "tfidf_lemmas"
INDEX_FILE = "index.txt"    # Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: page1.txt https://...

tokenizer = RegexpTokenizer(r'\b[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]{2,}\b')
morph = MorphAnalyzer()

stop_words = set(["Ğ¸", "Ğ²", "Ğ½Ğ°", "Ğ¿Ğ¾", "Ğ·Ğ°", "Ñ", "Ğº", "Ğ¸Ğ·", "Ğ¾Ñ‚", "Ñƒ", "Ğ¾", "Ğ°", "Ğ´Ğ¾", "Ğ½Ğµ", "Ñ‡Ñ‚Ğ¾", "Ğ¾Ğ½", "Ğ¾Ğ½Ğ°"])

# â”€â”€â”€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def tokenize(text):
    tokens = tokenizer.tokenize(text.lower())
    return [t for t in tokens if t not in stop_words]

def lemmatize(tokens):
    return [morph.parse(token)[0].normal_form for token in tokens]

# â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° index.txt Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ { 'page_1.txt': 'https://example.com', ... }

def load_index():
    index_map = {}
    with open(INDEX_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or ':' not in line:
                continue
            try:
                number, url = line.split(":", 1)
                number = number.strip()
                url = url.strip()
                filename = f"page_{number}.txt"
                index_map[filename] = url
            except ValueError:
                continue
    return index_map


def load_documents():
    docs = {}
    for filename in os.listdir(TFIDF_DIR):
        filepath = os.path.join(TFIDF_DIR, filename)
        tfidf = {}
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) == 3:
                    term, idf, tfidf_val = parts
                    tfidf[term] = float(tfidf_val)
        docs[filename] = tfidf
    return docs

def extract_idf(all_docs):
    idf_dict = {}
    for tfidf_dict in all_docs.values():
        for term, tfidf_val in tfidf_dict.items():
            if term not in idf_dict:
                tf = tfidf_val  # tf * idf = tfidf â†’ idf = tfidf / tf
                if tf > 0:
                    idf_dict[term] = tfidf_val / tf
    return idf_dict

# â”€â”€â”€ ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_vocab(docs):
    vocab = set()
    for tfidf_dict in docs.values():
        vocab.update(tfidf_dict)
    return sorted(vocab)

def vectorize(tfidf_dict, vocab):
    return np.array([tfidf_dict.get(term, 0.0) for term in vocab])

def cosine_similarity(vec1, vec2):
    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
        return 0.0
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def process_query(query, idf_dict):
    tokens = tokenize(query)
    lemmas = lemmatize(tokens)
    lemma_counts = Counter(lemmas)
    total = sum(lemma_counts.values())
    tf = {lemma: count / total for lemma, count in lemma_counts.items()}
    tfidf_query = {lemma: tf[lemma] * idf_dict.get(lemma, 0.0) for lemma in tf}
    return tfidf_query

# â”€â”€â”€ ĞŸĞ¾Ğ¸ÑĞº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search(query, top_k=5):
    doc_index = load_index()
    all_docs = load_documents()
    vocab = build_vocab(all_docs)
    idf_dict = extract_idf(all_docs)

    query_vector = vectorize(process_query(query, idf_dict), vocab)

    results = []
    for doc_name, tfidf_dict in all_docs.items():
        doc_vector = vectorize(tfidf_dict, vocab)
        score = cosine_similarity(query_vector, doc_vector)
        results.append((doc_name, score))

    results.sort(key=lambda x: x[1], reverse=True)

    print(f"\nğŸ” Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ: Â«{query}Â»\n")
    for doc_name, score in results[:top_k]:
        link = doc_index.get(doc_name, "ÑÑÑ‹Ğ»ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
        print(f"{doc_name:25} | score: {score:.4f} | {link}")

# â”€â”€â”€ Ğ¢Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    print("ğŸ” ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¸Ğº Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ TF-IDF Ğ¸Ğ· Ğ¿Ğ°Ğ¿ĞºĞ¸ tfidf_lemmas)\n")
    while True:
        query = input("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ (Ğ¸Ğ»Ğ¸ 'exit'): ").strip()
        if query.lower() == "exit":
            break
        search(query)
